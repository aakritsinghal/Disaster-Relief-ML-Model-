# Disaster-Relief-ML-Model-
Using and visualizing Word Vectors by GloVe Embeddings

# Synopsis
A shortcoming of the bag-of-words approach is that it only looks at the counts of words in each tweet. The idea of computationally extracting meaning from words is central to word vectors, which have become a cornerstone of modern deep learning on text. Word vectors are a mapping from words to vectors such that words that have similar meaning have similar word vectors. For example, the words "good" and "great" have similar word vectors, and the words "good" and "planet" have different word vectors. Thus, word vectors provide us a way to account for the meanings of words with our machine learning models.

I used GloVe embeddings to train a model by feeding a set of tweets to the model, which classifies them to a category. This is used to determine the need of various people after an event like a disaster since the categorized tweets are more helpful to determine the problem and solution
